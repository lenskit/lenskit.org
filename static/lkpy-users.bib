@article{ashokanFairnessMetricsBias2021,
  title = {Fairness Metrics and Bias Mitigation Strategies for Rating Predictions},
  author = {Ashokan, Ashwathy and Haas, Christian},
  year = {2021},
  month = sep,
  journal = {Inf. Process. Manag.},
  volume = {58},
  number = {5},
  pages = {102646},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2021.102646},
  abstract = {Algorithm fairness is an established line of research in the machine learning domain with substantial work while the equivalent in the recommender system domain is relatively new. In this article, we consider rating-based recommender systems which model the recommendation process as a prediction problem. We consider different types of biases that can occur in this setting, discuss various fairness definitions, and also propose a novel bias mitigation strategy to address potential unfairness in a rating-based recommender system. Based on an analysis of fairness metrics used in machine learning and a discussion of their applicability in the recommender system domain, we map the proposed metrics from the two domains and identify commonly used concepts and definitions of fairness. Finally, to address unfairness and potential bias against certain groups in a recommender system, we develop a bias mitigation algorithm and conduct case studies on one synthetic and one empirical dataset to show its effectiveness. Our results show that unfairness can be significantly lowered through our approach and that bias mitigation is a fruitful area of research for recommender systems.},
  keywords = {Algorithmic fairness,Bias mitigation,Fairness metrics,Recommender systems},
  file = {/Users/mde48/Zotero/storage/58AVJGPQ/Ashokan and Haas 2021 - Fairness metrics and bias mitigation strategies for rating predictions.pdf}
}

@article{balchanowskiCollaborativeRankAggregation2022,
  title = {Collaborative {{Rank Aggregation}} in {{Recommendation Systems}}},
  author = {Ba{\l}chanowski, Micha{\l} and Boryczka, Urszula},
  year = {2022},
  month = jan,
  journal = {Procedia Comput. Sci.},
  volume = {207},
  pages = {2213--2222},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2022.09.281},
  abstract = {Over the years, various techniques of generating recommendations have been developed. However, it turns out that when we compare the recommendations generated by different algorithms in the context of a particular user, the quality of such recommendations for different techniques may differ. The use of the aggregation techniques, the aim of which is to combine several rankings into one, can be a solution to this problem. In theory it should improve the quality of the recommendations. Additionally, in order to personalize the recommendations better, a metaheuristic algorithm, which, by assigning different weights to each feature, tries to represent the preference of the active user, was used. This paper also presents a suggestion to include additional rankings generated for other users in the system in the aggregation process. The idea will be supported by research results that clearly show that taking into account rankings of other users can improve the quality of the generated recommendations.},
  keywords = {differential evolution,metaheuristic,rank aggregation,recommendation systems},
  file = {/Users/mde48/Zotero/storage/GXNBIZAN/Bałchanowski and Boryczka 2022 - Collaborative Rank Aggregation in Recommendation Systems.pdf}
}

@article{balchanowskiComparativeStudyRank2023,
  title = {A {{Comparative Study}} of {{Rank Aggregation Methods}} in {{Recommendation Systems}}},
  author = {Ba{\l}chanowski, Micha{\l} and Boryczka, Urszula},
  year = {2023},
  month = jan,
  journal = {Entropy},
  volume = {25},
  number = {1},
  pages = {132},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1099-4300},
  doi = {10.3390/e25010132},
  urldate = {2023-01-12},
  abstract = {The aim of a recommender system is to suggest to the user certain products or services that most likely will interest them. Within the context of personalized recommender systems, a number of algorithms have been suggested to generate a ranking of items tailored to individual user preferences. However, these algorithms do not generate identical recommendations, and for this reason it has been suggested in the literature that the results of these algorithms can be combined using aggregation techniques, hoping that this will translate into an improvement in the quality of the final recommendation. In order to see which of these techniques increase the quality of recommendations to the greatest extent, the authors of this publication conducted experiments in which they considered five recommendation algorithms and 20 aggregation methods. The research was carried out on the popular and publicly available MovieLens 100k and MovieLens 1M datasets, and the results were confirmed by statistical tests.},
  file = {/Users/mde48/Zotero/storage/D5I9C7WW/Bałchanowski and Boryczka 2023 - A Comparative Study of Rank Aggregation Methods in Recommendation Systems.pdf}
}

@mastersthesis{balsecaninezEstudioEstadoArte2019,
  title = {Estudio Del Estado Del Arte de La Ciencia de Datos Aplicada a La Neuroeconom\'ia},
  author = {Balseca N{\'i}{\~n}ez, Christian Ricardo and Fern{\'a}ndez Pe{\~n}afiel, Daniel Andr{\'e}s},
  year = {2019},
  url = {http://dspace.ups.edu.ec/handle/123456789/17605},
  school = {Universidad Polit\'ecnica Salesiana}
}

@inproceedings{bauerFairRecKitWebbasedAnalysis2023,
  title = {{{FairRecKit}}: {{A Web-based Analysis Software}} for {{Recommender Evaluations}}},
  booktitle = {{{CHIIR}} '23},
  author = {Bauer, Christine and Chung, Lennard and Cornelissen, Aleksej and {van Driessel}, Isabelle and {van der Hoorn}, Diede and {de Jong}, Yme and Le, Lan and Najiyan Tabriz, Sanaz and Spaans, Roderick and Thijsen, Casper and Verbeeten, Robert and Wesseling, Vos and Wieland, Fern},
  year = {2023},
  month = mar,
  pages = {438--443},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3576840.3578274},
  urldate = {2023-03-28},
  abstract = {FairRecKit is a web-based analysis software that supports researchers in performing, analyzing, and understanding recommendation computations. The idea behind FairRecKit is to facilitate the in-depth analysis of recommendation outcomes considering fairness aspects. With (nested) filters on user or item attributes, metrics can easily be compared across user and item subgroups. Further, (nested) filters can be used on the dataset level; this way, recommendation outcomes can be compared across several sub-datasets to analyze for differences considering fairness aspects. The software currently features five datasets, 11 metrics, and 21 recommendation algorithms to be used in computational experimentation. It is open source and developed in a modular manner to facilitate extension. The analysis software consists of two components: A software package (FairRecKitLib) for running recommendation algorithms on the available datasets and a web-based user interface (FairRecKitApp) to start experiments, retrieve results of previous experiments, and analyze details. The application also comes with extensive documentation and options for result customization, which makes for a flexible tool that supports in-depth analysis.},
  keywords = {{evaluation, music, web-based, recommender systems, toolkit, analysis, movies, resource}},
  file = {/Users/mde48/Zotero/storage/ZZ2CMY4Y/Bauer et al. 2023 - FairRecKit - A Web-based Analysis Software for Recommender Evaluations.pdf}
}

@unpublished{bhattacharyaWhatYouGenerating2022,
  title = {What {{You Like}}: {{Generating Explainable Topical Recommendations}} for {{Twitter Using Social Annotations}}},
  author = {Bhattacharya, Parantapa and Ghosh, Saptarshi and Zafar, Muhammad Bilal and Ghosh, Soumya K and Ganguly, Niloy},
  year = {2022},
  month = dec,
  journal = {arXiv [cs.IR]},
  url = {http://arxiv.org/abs/2212.13897},
  abstract = {With over 500 million tweets posted per day, in Twitter, it is difficult for Twitter users to discover interesting content from the deluge of uninteresting posts. In this work, we present a novel, explainable, topical recommendation system, that utilizes social annotations, to help Twitter users discover tweets, on topics of their interest. A major challenge in using traditional rating dependent recommendation systems, like collaborative filtering and content based systems, in high volume social networks is that, due to attention scarcity most items do not get any ratings. Additionally, the fact that most Twitter users are passive consumers, with 44\% users never tweeting, makes it very difficult to use user ratings for generating recommendations. Further, a key challenge in developing recommendation systems is that in many cases users reject relevant recommendations if they are totally unfamiliar with the recommended item. Providing a suitable explanation, for why the item is recommended, significantly improves the acceptability of recommendation. By virtue of being a topical recommendation system our method is able to present simple topical explanations for the generated recommendations. Comparisons with state-of-the-art matrix factorization based collaborative filtering, content based and social recommendations demonstrate the efficacy of the proposed approach.},
  isbn = {2212.13897},
  file = {/Users/mde48/Zotero/storage/KBTXW2TW/Bhattacharya et al. 2022 - What You Like - Generating Explainable Topical Recommendations for Twitter Using Social Annotations.pdf}
}

@inproceedings{chenImpactEndUserPrivacy2020,
  title = {Impact of {{End-User Privacy Enhancing Technologies}} ({{PETs}}) on {{Firms}}' {{Analytics Performance}}},
  booktitle = {{{ICIS}} 2020 {{Proceedings}}},
  author = {Chen, Dawei and Hahn, Jungpil},
  year = {2020},
  url = {https://aisel.aisnet.org/icis2020/digital_commerce/digital_commerce/3/},
  urldate = {2021-01-28},
  abstract = {Big data analytics in digital commerce requires vast amounts of personal information from consumers, but this gives rise to major privacy concerns. To combat the threat of privacy invasion, individuals are proactively adopting privacy enhancing technologies (PETs) to protect their personal information. Consumers' adoption of PETs may hamper firms' big data analytics capabilities and performance but our knowledge of how PETs impact firms' data analytics is limited. This study proposes an inductively derived framework which qualitatively shows that end-user PETs induce measurement error and/or missing values with regards to attributes, entities and relationships in firms' customer databases, but the impacts of specific end-user PETs may vary by analytics use case. Our simulation experiments in the context of product recommendations quantitively find that consumers' adoption characteristics (adoption rate and pattern) and PETs characteristics (protection mechanism and intensity) significantly affect the performance of recommender systems.},
  file = {/Users/mde48/Zotero/storage/XVQEMMJP/Chen and Hahn 2020 - Impact of End-User Privacy Enhancing Technologies (PETs) on Firms’ Analytics Performance.pdf}
}

@article{depessemierEvaluatingFacialRecognition2020,
  title = {Evaluating Facial Recognition Services as Interaction Technique for Recommender Systems},
  author = {De Pessemier, Toon and Coppens, Ine and Martens, Luc},
  year = {2020},
  month = jun,
  journal = {Multimed. Tools Appl.},
  issn = {1380-7501},
  doi = {10.1007/s11042-020-09061-8},
  abstract = {Recommender systems are tools and techniques to assist users in the content selection process thereby coping with the problem of information overload. For recommender systems, user authentication and feedback gathering are of crucial importance. However, the typical user authentication with username / password and feedback method with a star rating system are not user friendly and often bypassed. This article proposes an alternative method for user authentication based on facial recognition and an automatic feedback gathering method by detecting various face characteristics such as emotions. We studied the use case of video watching. Photos made with the front-facing camera of a tablet, smartphone, or smart TV are used as input of a facial recognition service. The persons in front of the screen can be identified. During video watching, implicit feedback for the video content is automatically gathered through emotion recognition, attention measurements, and behavior analysis. An evaluation with a test panel showed that the recognized emotions are correlated with the user's star ratings and that happiness can be most accurately detected. So as the main contribution, this article indicates that emotion recognition might be used as an alternative feedback mechanism for recommender systems.}
}

@inproceedings{diazEvaluatingStochasticRankings2020,
  title = {Evaluating {{Stochastic Rankings}} with {{Expected Exposure}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Diaz, Fernando and Mitra, Bhaskar and Ekstrand, Michael D and Biega, Asia J and Carterette, Ben},
  year = {2020},
  month = oct,
  series = {{{CIKM}} '20},
  publisher = {{ACM}},
  doi = {10.1145/3340531.3411962},
  abstract = {We introduce the concept of expected exposure as the average attention ranked items receive from users over repeated samples of the same query. Furthermore, we advocate for the adoption of the principle of equal expected exposure: given a fixed information need, no item receive more or less expected exposure compared to any other item of the same relevance grade. We argue that this principle is desirable for many retrieval objectives and scenarios, including topical diversity and fair ranking. Leveraging user models from existing retrieval metrics, we propose a general evaluation methodology based on expected exposure and draw connections to related metrics in information retrieval evaluation. Importantly, this methodology relaxes classic information retrieval assumptions, allowing a system, in response to a query, to produce a distribution over rankings instead of a single fixed ranking. We study the behavior of the expected exposure metric and stochastic rankers across a variety of information access conditions, including ad hoc retrieval and recommendation. We believe that measuring and optimizing expected exposure metrics using randomization opens a new area for retrieval algorithm development and progress.},
  file = {/Users/mde48/Zotero/storage/KXIMPF6B/Diaz et al. 2020 - Evaluating Stochastic Rankings with Expected Exposure.pdf}
}

@inproceedings{dokoupilEasyStudyFrameworkEasy2023,
  title = {{{EasyStudy}}: {{Framework}} for {{Easy Deployment}} of {{User Studies}} on {{Recommender Systems}}},
  shorttitle = {{{EasyStudy}}},
  booktitle = {Proceedings of the 17th {{ACM Conference}} on {{Recommender Systems}}},
  author = {Dokoupil, Patrik and Peska, Ladislav},
  year = {2023},
  month = sep,
  series = {{{RecSys}} '23},
  pages = {1196--1199},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3604915.3610640},
  urldate = {2023-09-18},
  abstract = {Improvements in the recommender systems (RS) domain are not possible without a thorough way to evaluate and compare newly proposed approaches. User studies represent a viable alternative to online and offline evaluation schemes, but despite their numerous benefits, they are only rarely used. One of the main reasons behind this fact is that preparing a user study from scratch involves a lot of extra work on top of a simple algorithm proposal. To simplify this task, we propose EasyStudy, a modular framework built on the credo ``Make simple things fast and hard things possible''. It features ready-to-use datasets, preference elicitation methods, incrementally tuned baseline algorithms, study flow plugins, and evaluation metrics. As a result, a simple study comparing several RS can be deployed with just a few clicks, while more complex study designs can still benefit from a range of reusable components, such as preference elicitation. Overall, EasyStudy dramatically decreases the gap between the laboriousness of offline evaluation vs. user studies and, therefore, may contribute towards the more reliable and insightful user-centric evaluation of next-generation RS. The project repository is available from https://bit.ly/easy-study-repo.},
  isbn = {9798400702419},
  keywords = {evaluation frameworks,Recommender systems,user centric,user studies},
  file = {/Users/mde48/Zotero/storage/YHWTYXLF/Dokoupil and Peska - 2023 - EasyStudy Framework for Easy Deployment of User S.pdf}
}

@inproceedings{dokoupilEffectSimilarityMetric2023,
  title = {The {{Effect}} of {{Similarity Metric}} and {{Group Size}} on {{Outlier Selection}} \& {{Satisfaction}} in {{Group Recommender Systems}}},
  booktitle = {Adjunct {{Proceedings}} of the 31st {{ACM Conference}} on {{User Modeling}}, {{Adaptation}} and {{Personalization}}},
  author = {Dokoupil, Patrik and Peska, Ladislav},
  year = {2023},
  month = jun,
  series = {{{UMAP}} '23 {{Adjunct}}},
  pages = {296--301},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3563359.3597386},
  urldate = {2023-06-25},
  abstract = {Group recommender systems (GRS) are a specific case of recommender systems (RS), where recommendations are constructed to a group of users rather than an individual. GRS has diverse application areas including trip planning, recommending movies to watch together, or music in shared environments. However, due to the lack of large datasets with group decision-making feedback information, or even the group definitions, GRS approaches are often evaluated offline w.r.t. individual user feedback and artificially generated groups. These synthetic groups are usually constructed w.r.t. pre-defined group size and inter-user similarity metric. While numerous variants of synthetic group generation procedures were utilized so far, its impact on the evaluation results was not sufficiently discussed. In this paper, we address this research gap by investigating the impact of various synthetic group generation procedures, namely the usage of different user similarity metrics and the effect of group sizes. We consider them in the context of ``outlier vs. majority'' groups, where a group of similar users is extended with one or more diverse ones. Experimental results indicate a strong impact of the selected similarity metric on both the typical characteristics of selected outliers as well as the performance of individual GRS algorithms. Moreover, we show that certain algorithms better adapt to larger groups than others.},
  isbn = {978-1-4503-9891-6},
  keywords = {Group Recommender systems,Synthetic groups construction,User similarity metrics},
  file = {/Users/mde48/Zotero/storage/MRBMBLRU/Dokoupil and Peska - 2023 - The Effect of Similarity Metric and Group Size on .pdf}
}

@article{ekstrandDistributionallyInformedRecommenderSystem2023,
  title = {Distributionally-{{Informed Recommender System Evaluation}}},
  author = {Ekstrand, Michael D. and Carterette, Ben and Diaz, Fernando},
  year = {2023},
  month = aug,
  journal = {ACM Transactions on Recommender Systems},
  doi = {10.1145/3613455},
  urldate = {2023-09-07},
  abstract = {Current practice for evaluating recommender systems typically focuses on point estimates of user-oriented effectiveness metrics or business metrics, sometimes combined with additional metrics for considerations such as diversity and novelty. In this paper, we argue for the need for researchers and practitioners to attend more closely to various distributions that arise from a recommender system (or other information access system) and the sources of uncertainty that lead to these distributions. One immediate implication of our argument is that both researchers and practitioners must report and examine more thoroughly the distribution of utility between and within different stakeholder groups. However, distributions of various forms arise in many more aspects of the recommender systems experimental process, and distributional thinking has substantial ramifications for how we design, evaluate, and present recommender systems evaluation and research results. Leveraging and emphasizing distributions in the evaluation of recommender systems is a necessary step to ensure that the systems provide appropriate and equitably-distributed benefit to the people they affect.},
  copyright = {All rights reserved},
  keywords = {distributions,evaluation,exposure,statistics},
  annotation = {Just Accepted},
  file = {/Users/mde48/Zotero/storage/AL8X7FIJ/Ekstrand et al. - 2023 - Distributionally-Informed Recommender System Evalu.pdf}
}

@article{ekstrandExploringAuthorGender2021,
  title = {Exploring {{Author Gender}} in {{Book Rating}} and {{Recommendation}}},
  author = {Ekstrand, Michael D and Kluver, Daniel},
  year = {2021},
  month = jul,
  journal = {User Modeling and User-Adapted Interaction},
  volume = {31},
  number = {3},
  pages = {377--420},
  issn = {0924-1868},
  doi = {10.1007/s11257-020-09284-2},
  urldate = {2020-06-05}
}

@phdthesis{fortesEnhancingMultiObjectiveRecommendation2022,
  title = {Enhancing the {{Multi-Objective Recommendation}} from Three New Perspectives: Data Characterization, Risk-Sensitiveness, and Prioritization of the Objectives},
  author = {Fortes, Renaldo Silva},
  year = {2022},
  journal = {Ci\^encia da Computa\c{c}\~ao},
  volume = {Doctoral},
  url = {https://repositorio.ufmg.br/bitstream/1843/43915/1/Reinaldo_Silva_Fortes_Tese_UFMG.pdf},
  collaborator = {Gon{\c c}alves, Marcos Andr{\'e}},
  school = {Universidade Federal de Minas Gerais},
  file = {/Users/mde48/Zotero/storage/MARBUUM5/Fortes 2022 - Enhancing the Multi-Objective Recommendation from three ne ... cterization, risk-sensitiveness, and prioritization of the objectives.pdf}
}

@article{godinotMeasuringEffectCollaborative2023,
  title = {Measuring the Effect of Collaborative Filtering on the Diversity of Users' Attention},
  author = {Godinot, Augustin and Tarissan, Fabien},
  year = {2023},
  month = jan,
  journal = {Applied Network Science},
  volume = {8},
  number = {1},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {2364-8228},
  doi = {10.1007/s41109-022-00530-7},
  urldate = {2023-04-30},
  abstract = {AbstractWhile the ever-increasing emergence of online services has led to a growing interest in the development of recommender systems, the algorithms underpinning such systems have begun to be criticized for their role in limiting the variety of content exposed to users. In this context, the notion of diversity has been proposed as a way of mitigating the side effects resulting from the specialization of recommender systems. In this paper, using a well-known recommender system that makes use of collaborative filtering in the context of musical content, we analyze the diversity of recommendations generated through the lens of the recently proposed information network diversity measure. The results of our study offer significant insights into the effect of algorithmic recommendations. On the one hand, we show that the musical selections of a large proportion of users are diversified as a result of the recommendations. On the other hand, however, such improvements do not benefit all users. They are in fact mainly restricted to users with a low level of activity or whose past musical listening selections are very narrow. Through more in-depth investigations, we also discovered that while recommendations generally increase the variety of the songs recommended to users, they nonetheless fail to provide a balanced exposure to the different related categories.},
  file = {/Users/mde48/Zotero/storage/IL2ZQKQX/Godinot and Tarissan 2023 - Measuring the effect of collaborative filtering on the diversity of users’ attention.pdf}
}

@article{godinotMeasuringEffectCollaborative2023a,
  title = {Measuring the Effect of Collaborative Filtering on the Diversity of Users' Attention},
  author = {Godinot, Augustin and Tarissan, Fabien},
  year = {2023},
  month = dec,
  journal = {Applied Network Science},
  volume = {8},
  number = {1},
  pages = {1--18},
  publisher = {{SpringerOpen}},
  issn = {2364-8228},
  doi = {10.1007/s41109-022-00530-7},
  urldate = {2023-09-19},
  abstract = {While the ever-increasing emergence of online services has led to a growing interest in the development of recommender systems, the algorithms underpinning such systems have begun to be criticized for their role in limiting the variety of content exposed to users. In this context, the notion of diversity has been proposed as a way of mitigating the side effects resulting from the specialization of recommender systems. In this paper, using a well-known recommender system that makes use of collaborative filtering in the context of musical content, we analyze the diversity of recommendations generated through the lens of the recently proposed information network diversity measure. The results of our study offer significant insights into the effect of algorithmic recommendations. On the one hand, we show that the musical selections of a large proportion of users are diversified as a result of the recommendations. On the other hand, however, such improvements do not benefit all users. They are in fact mainly restricted to users with a low level of activity or whose past musical listening selections are very narrow. Through more in-depth investigations, we also discovered that while recommendations generally increase the variety of the songs recommended to users, they nonetheless fail to provide a balanced exposure to the different related categories.},
  copyright = {2023 The Author(s)},
  langid = {english},
  file = {/Users/mde48/Zotero/storage/DU7RZ62C/Godinot and Tarissan - 2023 - Measuring the effect of collaborative filtering on.pdf}
}

@unpublished{halpernRepresentationIncompleteVotes2022,
  title = {Representation with {{Incomplete Votes}}},
  author = {Halpern, Daniel and Kehne, Gregory and Procaccia, Ariel D and {Tucker-Foltz}, Jamie and W{\"u}thrich, Manuel},
  year = {2022},
  month = nov,
  journal = {arXiv [cs.GT]},
  url = {http://arxiv.org/abs/2211.15608},
  abstract = {Platforms for online civic participation rely heavily on methods for condensing thousands of comments into a relevant handful, based on whether participants agree or disagree with them. These methods should guarantee fair representation of the participants, as their outcomes may affect the health of the conversation and inform impactful downstream decisions. To that end, we draw on the literature on approval-based committee elections. Our setting is novel in that the approval votes are incomplete since participants will typically not vote on all comments. We prove that this complication renders non-adaptive algorithms impractical in terms of the amount of information they must gather. Therefore, we develop an adaptive algorithm that uses information more efficiently by presenting incoming participants with statements that appear promising based on votes by previous participants. We prove that this method satisfies commonly used notions of fair representation, even when participants only vote on a small fraction of comments. Finally, an empirical evaluation using real data shows that the proposed algorithm provides representative outcomes in practice.},
  isbn = {2211.15608},
  file = {/Users/mde48/Zotero/storage/T23GCGLU/Halpern et al. 2022 - Representation with Incomplete Votes.pdf}
}

@inproceedings{ihemelanduInferenceScaleSignificance2023,
  title = {Inference at {{Scale}}: {{Significance Testing}} for {{Large Search}} and {{Recommendation Experiments}}},
  shorttitle = {Inference at {{Scale}}},
  booktitle = {Proceedings of the 46th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Ihemelandu, Ngozi and Ekstrand, Michael D.},
  year = {2023},
  month = jul,
  series = {{{SIGIR}} '23},
  pages = {2087--2091},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3539618.3592004},
  urldate = {2023-07-23},
  abstract = {A number of information retrieval studies have been done to assess which statistical techniques are appropriate for comparing systems. However, these studies are focused on TREC-style experiments, which typically have fewer than 100 topics. There is no similar line of work for large search and recommendation experiments; such studies typically have thousands of topics or users and much sparser relevance judgements, so it is not clear if recommendations for analyzing traditional TREC experiments apply to these settings. In this paper, we empirically study the behavior of significance tests with large search and recommendation evaluation data. Our results show that the Wilcoxon and Sign tests show significantly higher Type-1 error rates for large sample sizes than the bootstrap, randomization and t-tests, which were more consistent with the expected error rate. While the statistical tests displayed differences in their power for smaller sample sizes, they showed no difference in their power for large sample sizes. We recommend the sign and Wilcoxon tests should not be used to analyze large scale evaluation results. Our result demonstrate that with Top-N recommendation and large search evaluation data, most tests would have a 100\% chance of finding statistically significant results. Therefore, the effect size should be used to determine practical or scientific significance.},
  copyright = {All rights reserved},
  isbn = {978-1-4503-9408-6},
  keywords = {evaluation,statistical inference},
  file = {/Users/mde48/Zotero/storage/4WFBEVLV/Ihemelandu and Ekstrand - 2023 - Inference at Scale Significance Testing for Large.pdf}
}

@misc{liMitigatingMainstreamBias2023,
  title = {Mitigating {{Mainstream Bias}} in {{Recommendation}} via {{Cost-sensitive Learning}}},
  author = {Li, Roger Zhe and Urbano, Juli{\'a}n and Hanjalic, Alan},
  year = {2023},
  month = jul,
  eprint = {2307.13632},
  primaryclass = {cs},
  doi = {10.1145/3578337.3605134},
  urldate = {2023-07-29},
  abstract = {Mainstream bias, where some users receive poor recommendations because their preferences are uncommon or simply because they are less active, is an important aspect to consider regarding fairness in recommender systems. Existing methods to mitigate mainstream bias do not explicitly model the importance of these non-mainstream users or, when they do, it is in a way that is not necessarily compatible with the data and recommendation model at hand. In contrast, we use the recommendation utility as a more generic and implicit proxy to quantify mainstreamness, and propose a simple user-weighting approach to incorporate it into the training process while taking the cost of potential recommendation errors into account. We provide extensive experimental results showing that quantifying mainstreamness via utility is better able at identifying non-mainstream users, and that they are indeed better served when training the model in a cost-sensitive way. This is achieved with negligible or no loss in overall recommendation accuracy, meaning that the models learn a better balance across users. In addition, we show that research of this kind, which evaluates recommendation quality at the individual user level, may not be reliable if not using enough interactions when assessing model performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval},
  file = {/Users/mde48/Zotero/storage/KT2HK99B/Li et al. - 2023 - Mitigating Mainstream Bias in Recommendation via C.pdf;/Users/mde48/Zotero/storage/ZLIXWP6X/2307.html}
}

@inproceedings{liNewInsightsMetric2021,
  title = {New {{Insights}} into {{Metric Optimization}} for {{Ranking-based Recommendation}}},
  booktitle = {{{SIGIR}} '21},
  author = {Li, Roger Zhe and Urbano, Juli{\'a}n and Hanjalic, Alan},
  year = {2021},
  month = jul,
  pages = {932--941},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3404835.3462973},
  urldate = {2021-07-15},
  abstract = {Direct optimization of IR metrics has often been adopted as an approach to devise and develop ranking-based recommender systems. Most methods following this approach (e.g. TFMAP, CLiMF, Top-N-Rank) aim at optimizing the same metric being used for evaluation, under the assumption that this will lead to the best performance. A number of studies of this practice bring this assumption, however, into question. In this paper, we dig deeper into this issue in order to learn more about the effects of the choice of the metric to optimize on the performance of a ranking-based recommender system. We present an extensive experimental study conducted on different datasets in both pairwise and listwise learning-to-rank (LTR) scenarios, to compare the relative merit of four popular IR metrics, namely RR, AP, nDCG and RBP, when used for optimization and assessment of recommender systems in various combinations. For the first three, we follow the practice of loss function formulation available in literature. For the fourth one, we propose novel loss functions inspired by RBP for both the pairwise and listwise scenario. Our results confirm that the best performance is indeed not necessarily achieved when optimizing the same metric being used for evaluation. In fact, we find that RBP-inspired losses perform at least as well as other metrics in a consistent way, and offer clear benefits in several cases. Interesting to see is that RBP-inspired losses, while improving the recommendation performance for all uses, may lead to an individual performance gain that is correlated with the activity level of a user in interacting with items. The more active the users, the more they benefit. Overall, our results challenge the assumption behind the current research practice of optimizing and evaluating the same metric, and point to RBP-based optimization instead as a promising alternative when learning to rank in the recommendation context.},
  keywords = {{evaluation metrics, learning to rank, recommender systems}},
  file = {/Users/mde48/Zotero/storage/QINLRZAP/Li et al. 2021 - New Insights into Metric Optimization for Ranking-based Recommendation.pdf}
}

@article{liuNewCollaborativeFiltering2022,
  title = {A {{New Collaborative Filtering Algorithm Integrating Time}} and {{Multisimilarity}}},
  author = {Liu, Qin},
  year = {2022},
  month = aug,
  journal = {Math. Probl. Eng.},
  volume = {2022},
  publisher = {{Hindawi}},
  issn = {1024-123X},
  doi = {10.1155/2022/2340671},
  urldate = {2022-09-05},
  abstract = {Aiming at the problem of low recommendation accuracy of existing recommendation algorithms, an algorithm integrating time factors and multisimilarity is proposed to improve the impact of long-term data, user attention, and project popularity on the recommendation algorithm and the similarity of user attributes is introduced to improve the problem of cold start to a certain extent. Considering that the longer the time, the less likely it is to be selected again, time is introduced into the algorithm as a weight factor. When the behavior occurs, i.e., interest in the project, so as to judge the similarity between users, not just the score value, we normalize the popularity to avoid misjudgment of high scoring and popular items. Because new users do not have past score records, the problem of cold start can be solved by calculating the similarity of user attributes. Through the comparative experiment on Movielens100K dataset and Epinions dataset, the results show that the algorithm can improve the accuracy of recommendation and give users a better recommendation effect.},
  file = {/Users/mde48/Zotero/storage/PQEISD7S/Liu 2022 - A New Collaborative Filtering Algorithm Integrating Time and Multisimilarity.pdf}
}

@unpublished{lopesRecommendationsMinimumExposure2022,
  title = {Recommendations with {{Minimum Exposure Guarantees}}: {{A Post-Processing Framework}}},
  author = {Lopes, Ramon and Alves, Rodrigo and Ledent, Antoine and Santos, Rodrygo and Kloft, Marius},
  year = {2022},
  month = nov,
  doi = {10.2139/ssrn.4274780},
  urldate = {2022-12-23},
  abstract = {Relevance-based ranking is a popular ingredient in recommenders, but it frequently struggles to meet fairness criteria because social and cultural norms may favor some item groups over others. For instance, some items might receive lower ratings due to some sort of bias (e.g. gender bias). A fair ranking should balance the exposure of items from advantaged and disadvantagedgroups. To this end, we propose a novel post-processing framework to produce fair, exposure-aware recommendations. Our approach is based on an integer linear programming model maximizing the expected utility while satisfying a minimum exposure constraint. The model has fewer variables than previous work and thus can be deployed to larger datasets and allows the organization to define a minimum level of exposure for groups of items. We conduct an extensive empirical evaluation indicating that our new framework can increase the exposure of items from disadvantaged groups at a small cost of recommendation accuracy.},
  keywords = {{recommender systems, fairness, exposure, integer linear programming}},
  file = {/Users/mde48/Zotero/storage/HDKYCNAN/Lopes et al. 2022 - Recommendations with Minimum Exposure Guarantees - A Post-Processing Framework.pdf}
}

@article{lopesRecommendationsMinimumExposure2024,
  title = {Recommendations with Minimum Exposure Guarantees: {{A}} Post-Processing Framework},
  shorttitle = {Recommendations with Minimum Exposure Guarantees},
  author = {Lopes, Ramon and Alves, Rodrigo and Ledent, Antoine and Santos, Rodrygo L. T. and Kloft, Marius},
  year = {2024},
  month = feb,
  journal = {Expert Systems with Applications},
  volume = {236},
  pages = {121164},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2023.121164},
  urldate = {2023-09-19},
  abstract = {Relevance-based ranking is a popular ingredient in recommenders, but it frequently struggles to meet fairness criteria because social and cultural norms may favor some item groups over others. For instance, some items might receive lower ratings due to some sort of bias (e.g. gender bias). A fair ranking should balance the exposure of items from advantaged and disadvantaged groups. To this end, we propose a novel post-processing framework to produce fair, exposure-aware recommendations. Our approach is based on an integer linear programming model maximizing the expected utility while satisfying a minimum exposure constraint. The model has fewer variables than previous work and thus can be deployed to larger datasets and allows the organization to define a minimum level of exposure for groups of items. We conduct an extensive empirical evaluation indicating that our new framework can increase the exposure of items from disadvantaged groups at a small cost of recommendation accuracy.},
  keywords = {Exposure,Fairness,Integer linear programming,Recommender systems}
}

@unpublished{lucheriniTRECSSimulationTool2021,
  title = {T-{{RECS}}: {{A}} Simulation Tool to Study the Societal Impact of Recommender Systems},
  author = {Lucherini, Eli and Sun, Matthew and Winecoff, Amy and Narayanan, Arvind},
  year = {2021},
  month = jul,
  journal = {arXiv [cs.CY]},
  url = {http://arxiv.org/abs/2107.08959},
  abstract = {Simulation has emerged as a popular method to study the long-term societal consequences of recommender systems. This approach allows researchers to specify their theoretical model explicitly and observe the evolution of system-level outcomes over time. However, performing simulation-based studies often requires researchers to build their own simulation environments from the ground up, which creates a high barrier to entry, introduces room for implementation error, and makes it difficult to disentangle whether observed outcomes are due to the model or the implementation. We introduce T-RECS, an open-sourced Python package designed for researchers to simulate recommendation systems and other types of sociotechnical systems in which an algorithm mediates the interactions between multiple stakeholders, such as users and content creators. To demonstrate the flexibility of T-RECS, we perform a replication of two prior simulation-based research on sociotechnical systems. We additionally show how T-RECS can be used to generate novel insights with minimal overhead. Our tool promotes reproducibility in this area of research, provides a unified language for simulating sociotechnical systems, and removes the friction of implementing simulations from scratch.},
  isbn = {2107.08959},
  file = {/Users/mde48/Zotero/storage/LXX4YFTW/Lucherini et al. 2021 - T-RECS - A simulation tool to study the societal impact of recommender systems.pdf}
}

@inproceedings{luStandingYourShoes2021,
  title = {Standing in {{Your Shoes}}: {{External Assessments}} for {{Personalized Recommender Systems}}},
  booktitle = {Proc. {{SIGIR}} 2021},
  author = {Lu, Hongyu and Ma, Weizhi and Zhang, Min and {de Rijke}, Maarten and Liu, Yiqun and Ma, Shaoping},
  year = {2021},
  doi = {10.1145/3404835.3462916},
  file = {/Users/mde48/Zotero/storage/52LAHTAG/Lu et al. 2021 - Standing in Your Shoes - External Assessments for Personalized Recommender Systems.pdf}
}

@article{luUserPerceptionRecommendation2022,
  title = {User {{Perception}} of {{Recommendation Explanation}}: {{Are Your Explanations What Users Need}}?},
  author = {Lu, Hongyu and Ma, Weizhi and Wang, Yifan and Zhang, Min and Wang, Xiang and Liu, Yiqun and Chua, Tat-Seng and Ma, Shaoping},
  year = {2022},
  month = nov,
  journal = {ACM Trans. Inf. Syst. Secur.},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  issn = {1094-9224},
  doi = {10.1145/3565480},
  abstract = {As recommender systems become increasingly important in daily human decision-making, users are demanding convincing explanations to understand they get the specific recommendation results. Although a number of explainable recommender systems have recently been proposed, there still lacks an understanding of what users really need in a recommendation explanation. The actual reason behind users' intention to examine and consume (e.g., click and watch a movie) can be the window to answer this question and is named as self-explanation in this work. In addition, humans usually make recommendations accompanied by explanations, but there remain fewer studies on how humans explain and what we can learn from human-generated explanations. To investigate these questions, we conduct a novel multi-role, multi-session user study in which users interact with multiple types of system-generated explanations as well as human-generated explanations, namely peer-explanation. During the study, users' intentions, expectations, and experiences are tracked in several phases, including before and after the users are presented with an explanation and after the content is examined. Through comprehensive investigations, three main findings have been made: First, we observe not only the positive but also the negative effects of explanations, and the impact varies across different types of explanations. Moreover, human-generated explanation, peer-explanation, performs better in increasing user intentions and helping users to better construct preferences, which results in better user satisfaction. Second, based on users' self-explanation, the information accuracy is measured and found to be a major factor associated with user satisfaction. Some other factors, such as unfamiliarity and similarity, are also discovered and summarized. Third, through annotations of the information aspects used in the human-generated self-explanation and peer-explanation, patterns of how humans explain are investigated, including what information and how much information is utilized. In addition, based on the findings, a human-inspired explanation approach is proposed and found to increase user satisfaction, revealing the potential improvement of further incorporating more human patterns in recommendation explanations. These findings have shed light on the deeper understanding of the recommendation explanation and further research on its evaluation and generation. Furthermore, the collected data, including human-generated explanations by both the external peers and the users' selves, will be released to support future research works on explanation evaluation.},
  keywords = {{Recommendation Explanation, User Modeling, Recommender System}},
  file = {/Users/mde48/Zotero/storage/8CW2MN47/Lu et al. 2022 - User Perception of Recommendation Explanation - Are Your Explanations What Users Need.pdf}
}

@unpublished{maoApplicationKnowledgeGraphs2021,
  title = {Application of {{Knowledge Graphs}} to {{Provide Side Information}} for {{Improved Recommendation Accuracy}}},
  author = {Mao, Yuhao and Mokhov, Serguei A and Mudur, Sudhir P},
  year = {2021},
  month = jan,
  journal = {arXiv [cs.IR]},
  doi = {10.1007/s11257-018-9213-x},
  abstract = {Personalized recommendations are popular in these days of Internet driven activities, specifically shopping. Recommendation methods can be grouped into three major categories, content based filtering, collaborative filtering and machine learning enhanced. Information about products and preferences of different users are primarily used to infer preferences for a specific user. Inadequate information can obviously cause these methods to fail or perform poorly. The more information we provide to these methods, the more likely it is that the methods perform better. Knowledge graphs represent the current trend in recording information in the form of relations between entities, and can provide additional (side) information about products and users. Such information can be used to improve nearest neighbour search, clustering users and products, or train the neural network, when one is used. In this work, we present a new generic recommendation systems framework, that integrates knowledge graphs into the recommendation pipeline. We describe its software design and implementation, and then show through experiments, how such a framework can be specialized for a domain, say movie recommendations, and the improvements in recommendation results possible due to side information obtained from knowledge graphs representation of such information. Our framework supports different knowledge graph representation formats, and facilitates format conversion, merging and information extraction needed for training recommendation methods.},
  isbn = {2101.03054},
  file = {/Users/mde48/Zotero/storage/GRUGYCI4/Mao et al. 2021 - Application of Knowledge Graphs to Provide Side Information for Improved Recommendation Accuracy.pdf}
}

@article{michielsFrameworkToolkitTesting2023,
  title = {A {{Framework}} and {{Toolkit}} for {{Testing}} the {{Correctness}} of {{Recommendation Algorithms}}},
  author = {Michiels, Lien and Verachtert, Robin and Ferraro, Andres and Falk, Kim and Goethals, Bart},
  year = {2023},
  month = apr,
  journal = {ACM Trans. Recomm. Syst.},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3591109},
  abstract = {Evaluating recommender systems adequately and thoroughly is an important task. Significant efforts are dedicated to proposing metrics, methods and protocols for doing so. However, there has been little discussion in the recommender systems' literature on the topic of testing. In this work, we adopt and adapt concepts from the software testing domain, e.g., code coverage, metamorphic testing, or property-based testing, to help researchers to detect and correct faults in recommendation algorithms. We propose a test suite that can be used to validate the correctness of a recommendation algorithm, and thus identify and correct issues that can affect the performance and behavior of these algorithms. Our test suite contains both black box and white box tests at every level of abstraction, i.e., system, integration and unit. To facilitate adoption, we release RecPack Tests, an open-source Python package containing template test implementations. We use it to test four popular Python packages for recommender systems: RecPack, PyLensKit, Surprise and Cornac. Despite the high test coverage of each of these packages, we find that we are still able to uncover undocumented functional requirements and even some bugs. This validates our thesis that testing the correctness of recommendation algorithms can complement traditional methods for evaluating recommendation algorithms.},
  file = {/Users/mde48/Zotero/storage/VRE2AFV6/Michiels et al. 2023 - A Framework and Toolkit for Testing the Correctness of Recommendation Algorithms.pdf}
}

@inproceedings{mudurFrameworkEnhancingDeep2021,
  title = {A {{Framework}} for {{Enhancing Deep Learning Based Recommender Systems}} with {{Knowledge Graphs}}},
  booktitle = {{{IDEAS}} 2021},
  author = {Mudur, Sudhir P and Mokhov, Serguei A and Mao, Yuhao},
  year = {2021},
  month = jul,
  pages = {11--20},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3472163.3472183},
  urldate = {2021-09-14},
  abstract = {Recommendation methods fall into three major categories, content based filtering, collaborative filtering and deep learning based. Information about products and the preferences of earlier users are used in an unsupervised manner to create models which help make personalized recommendations to a specific new user. The more information we provide to these methods, the more likely it is that they yield better recommendations. Deep learning based methods are relatively recent, and are generally more robust to noise and missing information. This is because deep learning models can be trained even when some of the information records have partial information. Knowledge graphs represent the current trend in recording information in the form of relations between entities, and can provide any available information about products and users. This information is used to train the recommendation model. In this work, we present a new generic recommender systems framework, that integrates knowledge graphs into the recommendation pipeline. We describe its design and implementation, and then show through experiments, how such a framework can be specialized, taking the domain of movies as an example, and the resulting improvements in recommendations made possible by using all the information obtained using knowledge graphs. Our framework, to be made publicly available, supports different knowledge graph representation formats, and facilitates format conversion, merging and information extraction needed for training recommendation models.},
  keywords = {{framework, knowledge graphs, recommendation model training, recommender system, deep learning based recommendations}},
  file = {/Users/mde48/Zotero/storage/KDRBFP9Q/Mudur et al. 2021 - A Framework for Enhancing Deep Learning Based Recommender Systems with Knowledge Graphs.pdf}
}

@mastersthesis{narayanWhatValueRating2019,
  title = {What Is the {{Value}} of {{Rating Obscure Items}}? {{An Analysis}} of the {{Effect}} of {{Less-Popular Items}} on {{Recommendation Quality}}},
  author = {Narayan, A},
  year = {2019},
  journal = {Department of Computer Science},
  volume = {M.S. in Computer Science},
  url = {https://conservancy.umn.edu/handle/11299/206148},
  abstract = {Recommender systems designers believe that the system stands to benefit from the users rating items that do not have many ratings. However, the effect of this act of rating lesser known items on the user's recommendations is unknown. This leads to asking the question of whether these low popularity items affect the recommendations received by users. This work looks at the effect less popular items have on a user's recommendations and the prediction and recommendations metrics that quantify the quality of recommendations \ldots},
  school = {University of Minnesota},
  file = {/Users/mde48/Zotero/storage/N4CLJGIM/Narayan 2019 - What is the Value of Rating Obscure Items - An Analysis of the Effect of Less-Popular Items on Recommendation Quality.pdf}
}

@article{ngCBRecBookRecommendation2020,
  title = {{{CBRec}}: A Book Recommendation System for Children Using the Matrix Factorisation and Content-Based Filtering Approaches},
  author = {Ng, Yiu-Kai},
  year = {2020},
  month = jan,
  journal = {International Journal of Business Intelligence and Data Mining},
  volume = {16},
  number = {2},
  pages = {129--149},
  publisher = {{Inderscience Publishers}},
  issn = {1743-8187},
  doi = {10.1504/IJBIDM.2020.104738},
  abstract = {Promoting good reading habits among children is essential, given the enormous influence of reading on students' development as learners and members of the society. Unfortunately, very few (children) websites or online applications recommend books to children, even though they can play a significant role in encouraging children to read. Given that a few popular book websites suggest books to children based on the popularity of books or rankings on books, they are not customised/personalised for each individual user and likely recommend books that users do not want or like. We have integrated the matrix factorisation approach and the content-based approach, in addition to predicting the grade levels of books, to recommend books for children. Recent research works have demonstrated that a hybrid approach, which combines different filtering approaches, is more effective in making recommendations. Conducted empirical study has verified the effectiveness of our proposed children book recommendation system.},
  file = {/Users/mde48/Zotero/storage/IJNP6LKJ/Ng 2020 - CBRec - a book recommendation system for children using the matrix factorisation and content-based filtering approaches.pdf}
}

@mastersthesis{nogueratorresSistemaRecomendacionMatricula2021,
  title = {Sistema de Recomendaci\'on de Matr\'icula de Cursos Electivos Para Estudiantes de {{Ingenier\'ia Electr\'onica}} e {{Ingenier\'ia}} de {{Telecomunicaciones}} de La {{UNAD}}},
  author = {Noguera Torres, Adriana del Pilar},
  year = {2021},
  month = apr,
  volume = {MS in Information Technology},
  url = {https://repository.unad.edu.co/handle/10596/40465},
  urldate = {2021-05-12},
  collaborator = {R{\'u}a P{\'e}rez, Santiago},
  school = {Universidad Nacional Abierta y a Distancia}
}

@article{pathakUnderstandingContributionRecommendation2023,
  title = {Understanding the {{Contribution}} of {{Recommendation Algorithms}} on {{Misinformation Recommendation}} and {{Misinformation Dissemination}} on {{Social Networks}}},
  author = {Pathak, Royal and Spezzano, Francesca and Pera, Maria Soledad},
  year = {2023},
  month = aug,
  journal = {ACM Transactions on the Web},
  issn = {1559-1131},
  doi = {10.1145/3616088},
  urldate = {2023-09-19},
  abstract = {Social networks are a platform for individuals and organizations to connect with each other and inform, advertise, spread ideas, and ultimately influence opinions. These platforms have been known to propel misinformation. We argue that this could be compounded by the recommender algorithms that these platforms use to suggest items potentially of interest to their users, given the known biases and filter bubbles issues affecting recommender systems. While much has been studied about misinformation on social networks, the potential exacerbation that could result from recommender algorithms in this environment is in its infancy. In this manuscript, we present the result of an in-depth analysis conducted on two datasets (Politifact FakeNewsNet dataset and HealthStory FakeHealth dataset) in order to deepen our understanding of the interconnection between recommender algorithms and misinformation spread on Twitter. In particular, we explore the degree to which well-known recommendation algorithms are prone to be impacted by misinformation. Via simulation, we also study misinformation diffusion on social networks, as triggered by suggestions produced by these recommendation algorithms. Outcomes from this work evidence that misinformation does not equally affect all recommendation algorithms. Popularity-based and network-based recommender algorithms contribute the most to misinformation diffusion. Users who are known to be superspreaders are known to directly impact algorithmic performance and misinformation spread in specific scenarios. Findings emerging from our exploration result in a number of implications for researchers and practitioners to consider when designing and deploying recommender algorithms in social networks.},
  keywords = {diffusion,misinformation,news,recommendation algorithms,social networks,Twitter},
  annotation = {Just Accepted},
  file = {/Users/mde48/Zotero/storage/4P9NDXDF/Pathak et al. - 2023 - Understanding the Contribution of Recommendation A.pdf}
}

@inproceedings{paullierRecommenderSystemsAlgorithm2020,
  title = {A {{Recommender Systems}}' Algorithm Evaluation Using the {{Lenskit}} Library and {{MovieLens}} Databases},
  booktitle = {2020 {{IEEE International Symposium}} on {{Broadband Multimedia Systems}} and {{Broadcasting}} ({{BMSB}})},
  author = {Paullier, Alejo and Sotelo, Rafael},
  year = {2020},
  month = oct,
  pages = {1--7},
  doi = {10.1109/BMSB49480.2020.9379914},
  abstract = {A wide variety of algorithms can be found in the recommender system's literature for computing predictions and obtaining recommendation lists. These algorithms' performance can be evaluated by numerous metrics depending on which business goal is intended to be optimized. However, performance can vary drastically depending on the selected algorithm as well as the selected metric. In addition, algorithmic performance can be heavily influenced by the experimental setup such as: dataset, cross-validation strategy, recommender framework and the model hyper-parameters chosen. In this work we will compare nine different algorithms used for generating recommendations across seven different metrics. One vital aspect of our analysis is to provide a detailed and clear explanation of the methodology carried out as well as the experimental setup in order to ensure future reproducibility. Moreover, performance and computation times will be introduced for further comparison. Finally, optimal model hyper-parameter combinations will be presented for benchmarking purposes.},
  keywords = {Algorithms,Business,Computational modeling,Databases,Evaluation,LensKit,Libraries,MovieLens,Multimedia communication,Prediction algorithms,Recommender systems,Reproducibility of results}
}

@inproceedings{pessemierUsingFacialRecognition2019,
  title = {Using Facial Recognition Services as Implicit Feedback for Recommenders},
  booktitle = {Proceedings of the 6th {{Joint Workshop}} on {{Interfaces}} and {{Human Decision Making}} for {{Recommender Systems}}},
  author = {Pessemier, Toon De and Coppens, Ine and Martens, Luc},
  year = {2019},
  volume = {2450},
  pages = {8},
  publisher = {{CEUR-WS}},
  url = {http://ceur-ws.org/Vol-2450/paper4.pdf},
  abstract = {User authentication and feedback gathering are crucial aspects for recommender systems. The most common implementations, a username / password login and star rating systems, require user interaction and a cognitive effort from the user. As a result, users opt to save their password in the interface and optional feedback with a star rating system is often skipped, especially for applications such as video watching in a home environment. In this article, we propose an alternative method for user authentication based on facial recognition and an automatic feedback gathering method by detecting various face characteristics. Using facial recognition with a camera in a tablet, smartphone, or smart TV, the persons in front of the screen can be identified in order to link video watching sessions to their user profile. During video watching, implicit feedback is automatically gathered through emotion recognition, attention measurements, and behavior analysis. An emotion fingerprint, which is defined as a unique spectrum of expected emotions for a video scene, is compared to the recognized emotions in order to estimate the experience of a user while watching. An evaluation with a test panel showed that happiness can be most accurately detected and the recognized emotions are correlated with the user's star rating.},
  file = {/Users/mde48/Zotero/storage/MAXSZ4DL/Pessemier et al. 2019 - Using facial recognition services as implicit feedback for recommenders.pdf}
}

@inproceedings{quang-hungExploringSetInspiredSimilarity2021,
  title = {Exploring {{Set-Inspired Similarity Measures}} for {{Collaborative Filtering Recommendation}}},
  booktitle = {Proceedings of the {{Conference}} on {{Knowledge}} and {{Software Engineering}}},
  author = {{Quang-Hung}, L E and {Thi-Xinh}, L E},
  year = {2021},
  month = nov,
  url = {https://iisi.siit.tu.ac.th/KSE2021/uploads_final/79__e291caf8e3a4ca8c47e9ed657a1d0f76/[KSE2021-0079]-Final-camera-ready-paper.pdf},
  file = {/Users/mde48/Zotero/storage/PZZIYS6A/Quang-Hung and Thi-Xinh 2021 - Exploring Set-Inspired Similarity Measures for Collaborative Filtering Recommendation.pdf}
}

@inproceedings{rajComparingFairRanking2020,
  title = {Comparing {{Fair Ranking Metrics}}},
  booktitle = {3rd {{FAccTRec Workshop}} on {{Responsible Recommendation}}},
  author = {Raj, Amifa and Wood, Connor and Montoly, Ananda and Ekstrand, Michael D},
  year = {2020},
  month = sep,
  url = {http://arxiv.org/abs/2009.01311},
  abstract = {Ranking is a fundamental aspect of recommender systems. However, ranked outputs can be susceptible to various biases; some of these may cause disadvantages to members of protected groups. Several metrics have been proposed to quantify the (un)fairness of rankings, but there has not been to date any direct comparison of these metrics. This complicates deciding what fairness metrics are applicable for specific scenarios, and assessing the extent to which metrics agree or disagree. In this paper, we describe several fair ranking metrics in a common notation, enabling direct comparison of their approaches and assumptions, and empirically compare them on the same experimental setup and data set. Our work provides a direct comparative analysis identifying similarities and differences of fair ranking metrics selected for our work.},
  file = {/Users/mde48/Zotero/storage/3IGTPAVF/Raj et al. 2020 - Comparing Fair Ranking Metrics.pdf}
}

@inproceedings{rajMeasuringFairnessRanked2022,
  title = {Measuring {{Fairness}} in {{Ranked Results}}: {{An Analytical}} and {{Empirical Comparison}}},
  booktitle = {Proceedings of the 45th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Raj, Amifa and Ekstrand, Michael D},
  year = {2022},
  month = jul,
  series = {{{SIGIR}} '22},
  pages = {726--736},
  publisher = {{ACM}},
  doi = {10.1145/3477495.3532018}
}

@inproceedings{scheidtTimedependentEvaluationRecommender2021,
  title = {Time-Dependent Evaluation of Recommender Systems},
  booktitle = {{{CEUR-WS}}},
  author = {Scheidt, Teresa and Beel, Joeran},
  year = {2021},
  month = sep,
  volume = {2955},
  url = {https://perspectives-ws.github.io/2021/proceedings/paper10.pdf},
  urldate = {2021-09-27},
  file = {/Users/mde48/Zotero/storage/QFVPZHZU/Scheidt and Beel 2021 - Time-dependent evaluation of recommender systems.pdf}
}

@article{slokomUserorientedPrivacyRecommender2021,
  title = {Towards User-Oriented Privacy for Recommender System Data: {{A}} Personalization-Based Approach to Gender Obfuscation for User Profiles},
  author = {Slokom, Manel and Hanjalic, Alan and Larson, Martha},
  year = {2021},
  month = nov,
  journal = {Inf. Process. Manag.},
  volume = {58},
  number = {6},
  pages = {102722},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2021.102722},
  abstract = {In this paper, we propose a new privacy solution for the data used to train a recommender system, i.e., the user\textendash item matrix. The user\textendash item matrix contains implicit information, which can be inferred using a classifier, leading to potential privacy violations. Our solution, called Personalized Blurring (PerBlur), is a simple, yet effective, approach to adding and removing items from users' profiles in order to generate an obfuscated user\textendash item matrix. The novelty of PerBlur is personalization of the choice of items used for obfuscation to the individual user profiles. PerBlur is formulated within a user-oriented paradigm of recommender system data privacy that aims at making privacy solutions understandable, unobtrusive, and useful for the user. When obfuscated data is used for training, a recommender system algorithm is able to reach performance comparable to what is attained when it is trained on the original, unobfuscated data. At the same time, a classifier can no longer reliably use the obfuscated data to predict the gender of users, indicating that implicit gender information has been removed. In addition to introducing PerBlur, we make several key contributions. First, we propose an evaluation protocol that creates a fair environment to compare between different obfuscation conditions. Second, we carry out experiments that show that gender obfuscation impacts the fairness and diversity of recommender system results. In sum, our work establishes that a simple, transparent approach to gender obfuscation can protect user privacy while at the same time improving recommendation results for users by maintaining fairness and enhancing diversity.},
  keywords = {Diversity,Evaluation,Fairness,Gender inference,Obfuscation,Privacy,Top-N recommendation},
  file = {/Users/mde48/Zotero/storage/PLJCEM8G/Slokom et al. 2021 - Towards user-oriented privacy for recommender syste ... ersonalization-based approach to gender obfuscation for user profiles.pdf}
}

@inproceedings{tianEstimatingErrorBias2020,
  title = {Estimating {{Error}} and {{Bias}} in {{Offline Evaluation Results}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Human Information Interaction}} and {{Retrieval}}},
  author = {Tian, Mucun and Ekstrand, Michael D},
  year = {2020},
  month = mar,
  series = {{{CHIIR}} '20},
  pages = {392--396},
  publisher = {{ACM}},
  doi = {10.1145/3343413.3378004},
  urldate = {2020-03-26},
  keywords = {{offline evaluation, simulation}},
  file = {/Users/mde48/Zotero/storage/B87NVXZV/Tian and Ekstrand 2020 - Estimating Error and Bias in Offline Evaluation Results.pdf}
}

@incollection{vargaRecommenderSystems2019,
  title = {Recommender {{Systems}}},
  booktitle = {Practical {{Data Science}} with {{Python}} 3},
  author = {Varga, Ervin},
  year = {2019},
  pages = {317--339},
  publisher = {{Apress}},
  doi = {10.1007/978-1-4842-4859-1},
  isbn = {978-1-4842-4858-4}
}

@inproceedings{venteIntroducingLensKitAutoExperimental2023,
  title = {Introducing {{LensKit-Auto}}, an {{Experimental Automated Recommender System}} ({{AutoRecSys}}) {{Toolkit}}},
  booktitle = {Proceedings of the 17th {{ACM Conference}} on {{Recommender Systems}}},
  author = {Vente, Tobias and Ekstrand, Michael and Beel, Joeran},
  year = {2023},
  month = sep,
  series = {{{RecSys}} '23},
  pages = {1212--1216},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3604915.3610656},
  urldate = {2023-09-18},
  abstract = {LensKit is one of the first and most popular Recommender System libraries. While LensKit offers a wide variety of features, it does not include any optimization strategies or guidelines on how to select and tune LensKit algorithms. LensKit developers have to manually include third-party libraries into their experimental setup or implement optimization strategies by hand to optimize hyperparameters. We found that 63.6\% (21 out of 33) of papers using LensKit algorithms for their experiments did not select algorithms or tune hyperparameters. Non-optimized models represent poor baselines and produce less meaningful research results. This demo introduces LensKit-Auto. LensKit-Auto automates the entire Recommender System pipeline and enables LensKit developers to automatically select, optimize, and ensemble LensKit algorithms.},
  isbn = {9798400702419},
  keywords = {Algorithm Selection,Automated Recommender Systems,AutoRecSys,CASH,Hyperparameter Optimization,Recommender Systems},
  file = {/Users/mde48/Zotero/storage/PCVGM9UC/Vente et al. - 2023 - Introducing LensKit-Auto, an Experimental Automate.pdf}
}

@inproceedings{wangBayesianDeepLearning2019,
  title = {Bayesian {{Deep Learning Based Exploration-Exploitation}} for {{Personalized Recommendations}}},
  booktitle = {2019 {{IEEE}} 31st {{International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}})},
  author = {Wang, X and Kadioglu, S},
  year = {2019},
  month = nov,
  pages = {1715--1719},
  publisher = {{ieeexplore.ieee.org}},
  doi = {10.1109/ICTAI.2019.00253},
  abstract = {Personalized Recommendation Systems require an effective method to balance exploration and exploitation. To learn effective strategies, user and item attributes are critical data sources to capture contextual information. In this paper, we first present an approach based on Bayesian Deep Learning to learn a compact representation of user and item attributes to guide exploitation. A key novelty of the approach lies in its ability to also capture the uncertainty associated with the model output to guide exploration. We then show how to further boost exploration by incorporating model uncertainty with that of data uncertainty. Experimental results demonstrate the benefits of our approach in terms of accuracy in recommendations as well as its performance in an online setting.},
  keywords = {Bayesian deep learning,{Bayesian Deep Learning, ExplorationExploitation, Personalized Recommendation},boost exploration,compact representation,contextual information,critical data sources,data uncertainty,exploration-exploitation,information filtering,item attributes,learning (artificial intelligence),model output,model uncertainty,personalized recommendation systems,recommender systems}
}

@article{wangModelingUncertaintyImprove2021,
  title = {Modeling Uncertainty to Improve Personalized Recommendations via {{Bayesian}} Deep Learning},
  author = {Wang, Xin and Kad{\i}o{\u g}lu, Serdar},
  year = {2021},
  month = mar,
  journal = {International Journal of Data Science and Analytics},
  issn = {2364-4168},
  doi = {10.1007/s41060-020-00241-1},
  abstract = {Modeling uncertainty has been a major challenge in developing Machine Learning solutions to solve real world problems in various domains. In Recommender Systems, a typical usage of uncertainty is to balance exploration and exploitation, where the uncertainty helps to guide the selection of new options in exploration. Recent advances in combining Bayesian methods with deep learning enable us to express uncertain status in deep learning models. In this paper, we investigate an approach based on Bayesian deep learning to improve personalized recommendations. We first build deep learning architectures to learn useful representation of user and item inputs for predicting their interactions. We then explore multiple embedding components to accommodate different types of user and item inputs. Based on Bayesian deep learning techniques, a key novelty of our approach is to capture the uncertainty associated with the model output and further utilize it to boost exploration in the context of Recommender Systems. We test the proposed approach in both a Collaborative Filtering and a simulated online recommendation setting. Experimental results on publicly available benchmarks demonstrate the benefits of our approach in improving the recommendation performance.},
  file = {/Users/mde48/Zotero/storage/TRN6UEEJ/Wang and Kadıoğlu 2021 - Modeling uncertainty to improve personalized recommendations via Bayesian deep learning.pdf}
}

@inproceedings{wegmethCaMeLSCooperativeMetalearning2022,
  title = {{{CaMeLS}}: {{Cooperative}} Meta-Learning Service for Recommender Systems},
  booktitle = {Proceedings of the {{Perspectives}} on the {{Evaluation}} of {{Recommender Systems Workshop}} 2022},
  author = {Wegmeth, Lukas and Beel, Joeran},
  year = {2022},
  month = sep,
  publisher = {{CEUR-WS}},
  url = {https://ceur-ws.org/Vol-3228/paper2.pdf},
  urldate = {2022-11-13},
  abstract = {We present CaMeLS, a proof of concept of a cooperative meta-learning service for recommender systems. CaMeLS leverages the computing power of recommender systems users by uploading their metadata and algorithm evaluation scores to a centralized environment. Through the resulting database, CaMeLS then offers meta-learning services for everyone. Additionally, users may access evaluations of common data sets immediately to know the best-performing algorithms for those data sets. The metadata table may also be used for other purposes, e.g., to perform benchmarks. In the initial version discussed in this paper, CaMeLS implements automatic algorithm selection through meta-learning over two recommender systems libraries. Automatic algorithm selection saves users time and computing power and does not require expertise, as the best algorithm is automatically found over multiple libraries. The CaMeLS database contains 20 metadata sets by default. We show that the automatic algorithm selection service is already on par with the single best algorithm in this default scenario. CaMeLS only requires a few seconds to predict a suitable algorithm, rather than potentially hours or days if performed manually, depending on the data set. The code is publicly available on our GitHub https://camels.recommender-systems.com.}
}

@inproceedings{wegmethEffectRandomSeeds2023,
  title = {The {{Effect}} of {{Random Seeds}} for {{Data Splitting}} on {{Recommendation Accuracy}}},
  booktitle = {Perspectives on the {{Evaluation}} of {{Recommender Systems Workshop}} ({{PERSPECTIVES}} 2023)},
  author = {Wegmeth, Lukas and Vente, Tobias and Purucker, Lennart and Beel, Joeran},
  year = {2023},
  month = sep,
  abstract = {The evaluation of recommender system algorithms depends on randomness, e.g., during randomly splitting data into training and testing data. We suspect that failing to account for randomness in this scenario may lead to misrepresenting the predictive accuracy of recommendation algorithms. To understand the community's view of the importance of randomness, we conducted a paper study on 39 full papers published at the ACM RecSys 2022 conference. We found that the authors of 26 papers used some variation of a holdout split that requires a random seed. However, only five papers explicitly repeated experiments and averaged their results over different random seeds. This potentially problematic research practice motivated us to analyze the effect of data split random seeds on recommendation accuracy. Therefore, we train three common algorithms on nine public data sets with 20 data split random seeds, evaluate them on two ranking metrics with three different ranking cutoff values {$\mathsl{k}$}, and compare the results. In the extreme case with {$\mathsl{k}$} = 1, we show that depending on the data split random seed, the accuracy with traditional recommendation algorithms deviates by up to {$\sim$}6.3\% from the mean accuracy achieved on the data set. Hence, we show that an algorithm may significantly over- or under-perform when maliciously or negligently selecting a random seed for splitting the data. To showcase a mitigation strategy and better research practice, we compare holdout to cross-validation and show that, again, for {$\mathsl{k}$} = 1, the accuracy of algorithms evaluated with cross-validation deviates only up to {$\sim$}2.3\% from the mean accuracy achieved on the data set. Furthermore, we found that the deviation becomes smaller the higher the value of {$\mathsl{k}$} for both holdout and cross-validation.},
  langid = {english},
  keywords = {to-read},
  file = {/Users/mde48/Zotero/storage/W764ZBF6/Wegmeth et al. - The Effect of Random Seeds for Data Splitting on R.pdf}
}

@unpublished{wegmethImpactFeatureQuantity2022,
  title = {The {{Impact}} of {{Feature Quantity}} on {{Recommendation Algorithm Performance}}: {{A Movielens-100K Case Study}}},
  author = {Wegmeth, Lukas},
  year = {2022},
  month = jul,
  journal = {arXiv [cs.IR]},
  url = {http://arxiv.org/abs/2207.08713},
  abstract = {Recent model-based Recommender Systems (RecSys) algorithms emphasize on the use of features, also called side information, in their design similar to algorithms in Machine Learning (ML). In contrast, some of the most popular and traditional algorithms for RecSys solely focus on a given user-item-rating relation without including side information. The goal of this case study is to provide a performance comparison and assessment of RecSys and ML algorithms when side information is included. We chose the Movielens-100K data set since it is a standard for comparing RecSys algorithms. We compared six different feature sets with varying quantities of features which were generated from the baseline data and evaluated on a total of 19 RecSys algorithms, baseline ML algorithms, Automated Machine Learning (AutoML) pipelines, and state-of-the-art RecSys algorithms that incorporate side information. The results show that additional features benefit all algorithms we evaluated. However, the correlation between feature quantity and performance is not monotonous for AutoML and RecSys. In these categories, an analysis of feature importance revealed that the quality of features matters more than quantity. Throughout our experiments, the average performance on the feature set with the lowest number of features is about 6\% worse compared to that with the highest in terms of the Root Mean Squared Error. An interesting observation is that AutoML outperforms matrix factorization-based RecSys algorithms when additional features are used. Almost all algorithms that can include side information have higher performance when using the highest quantity of features. In the other cases, the performance difference is negligible ({$<$}1\%). The results show a clear positive trend for the effect of feature quantity as well as the important effects of feature quality on the evaluated algorithms.},
  isbn = {2207.08713},
  file = {/Users/mde48/Zotero/storage/834PKEK2/Wegmeth 2022 - The Impact of Feature Quantity on Recommendation Algorithm Performance - A Movielens-100K Case Study.pdf}
}

@incollection{zieglerEmpfehlungssysteme2020,
  title = {Empfehlungssysteme},
  booktitle = {Handbuch {{Digitale Wirtschaft}}},
  author = {Ziegler, J{\"u}rgen and Loepp, Benedikt},
  editor = {Kollmann, Tobias},
  year = {2020},
  pages = {717--741},
  publisher = {{Springer Fachmedien Wiesbaden}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-17291-6_52},
  abstract = {Empfehlungssysteme stellen heute eine zentrale Komponente vieler Online-Plattformen dar, die bei Online-Shops und vielen anderen Anwendungen h\"aufig zum Einsatz kommt. Ziel ist es, dem Kunden entsprechend seinen pers\"onlichen Pr\"aferenzen Produkte oder andere Artikel vorzuschlagen, die f\"ur ihn von Interesse sind und potenziell zu einem Kauf oder generell zur Nutzung f\"uhren. Empfehlungssysteme haben eine erhebliche wirtschaftliche Bedeutung, da sie in vielen F\"allen zu einem signifikanten Anteil zu Erfolgsfaktoren wie Click-through-Raten oder K\"aufen beitragen. Wir stellen in diesem Kapitel die unterschiedlichen Ans\"atze zur automatisierten Empfehlungsgebung vor und beschreiben konkrete Techniken zu deren Umsetzung. Weiterhin gehen wir auf wesentliche Aspekte der Gestaltung und Bewertung von Empfehlungssystemen ein und diskutieren anwenderrelevante Themen wie Usability und Vertrauen in systemgenerierte Empfehlungen.},
  isbn = {978-3-658-17291-6}
}
